[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Medical Statistics with Jamovi",
    "section": "",
    "text": "This is a working draft.\nThis textbook is for medical students, doctors, medical researchers, nurses, members of professions allied to medicine, and all others concerned with medical data.\nWhile statistics books focus on mathematics, this textbook focuses on using a computer to conduct data analysis. That means using a statistical software program, in this case the Jamovi software for statistics and graphics. Our aim is to keep a balance between mathematical rigor and readability as well as learning Jamovi and statistics simultaneously.\nMost of the examples discussed in this textbook are based on scientific studies whose data are publicly available. For each example, we provide the step-by-step application in Jamovi. Readers are encouraged to follow these steps while reading the textbook so that they can learn statistics through hands-on experience.\nAll sections of this textbook are reproducible as they were made using Quarto® which is an open-source scientific and technical publishing system built on Pandoc.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\nThis textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Although some healthcare professionals may not carry out medical research, they will definitely be consumers of medical research. Thus, it is incumbent on them to be able to discern high quality research studies from low quality, to be able to verify whether the conclusions of a study are valid and to understand the limitations in methods of a study. The current emphasis on evidence-based medicine (EBM) requires that healthcare professionals consider critically all evidence about whether a specific treatment works and this requires basic statistical knowledge.\nStatistics is not only a discipline in its own right but it is also a fundamental tool for investigation in all biological and medical sciences. As such, any serious investigator in these fields must have a grasp of the basic principles. With modern computer facilities there is little need for familiarity with the technical details of statistical calculations. However, a healthcare professional should understand when such calculations are valid, when they are not and how they should be interpreted.\nThe use of statistical methods pervades the medical literature. In a survey of 350 original articles published in three UK journals of general practice: British Medical Journal (General Practice Section), British Journal of General Practice and Family Practice, over a one-year period, Rigby et al. (2004) found that 66% used some form of statistical analysis. Another review by Strasak et al. (2007) of 91 original research articles published in The New England Journal of Medicine (one of the prestigious peer-reviewed medical journals) found an even higher percentage (95%) of using inferential statistics, for example, hypothesis testing and deriving estimates. It appears, therefore, that the majority of papers published in these journals require some statistical knowledge for a complete understanding.\nTo students schooled in the ‘hard’ sciences of physics and chemistry it may be difficult to appreciate the variability of biological data. If one repeatedly puts blue litmus paper into acid solutions it turns red 100% of the time, not most (say 95%) of the time. In contrast, if one gives aspirin to a group of people with headaches, not all of them will experience relief. Penicillin was perhaps one of the few ‘miracle’ cures where the results were so dramatic that little evaluation was required. Absolute certainty in medicine is rare.\nMeasurements on human subjects seldom give exactly the same results from one occasion to the next. For example, O’ Sullivan et al (1999), found that systolic blood pressure (SBP) in normal healthy children has a wide range, with 95% of children having SBPs below 130 mmHg when they were resting, rising to 160 mmHg during the school day, and falling again to below 130 mmHg at night. Furthermore, Hansen et al. (2010) in a study of over 8000 subjects found that increasing variability in blood pressure over 24 hours was a significant and independent predictor of mortality and a cardiovascular and stroke events.\nThis variability is also inherent in responses to biological hazards. Most people now accept that cigarette smoking causes lung cancer and heart disease, and yet nearly everyone can point to an apparently healthy 80-year-old who has smoked for many years without apparent ill effect. Although it is now known from the report of Doll et al (2004) that about half of all persistent cigarette smokers are killed by their habit, it is usually forgotten that until the 1950s, the cause of the rise in lung cancer deaths was a mystery and commonly associated with general atmospheric pollution such as the exhaust fumes of cars. It was not until the carefully designed and statistically analysed case–control and cohort studies of Richard Doll and Austin Bradford Hill and others, that smoking was identified as the true cause. Enstrom et al. (2003) moved the debate on to ask whether or not passive smoking causes lung cancer. This is a more difficult question to answer since the association is weaker. However, studies by Cao et al. (2015) have now shown that it is a major health problem and scientists at the International Agency for Rsearch on Cancer (IARC) have concluded that there is sufficient evidence that second-hand smoke causes lung cancer (IARC 2012). Restrictions on smoking in public places have been imposed to smokers.\nWith such variability, it follows that in any comparison made in a medical context, such as people on different treatments, differences are almost bound to occur. These differences may be due to real effects, random variation or variation in some other factor that may affect an outcome. It is the job of the analyst to decide how much variation should be ascribed to chance or other factors, so that any remaining variation can be assumed to be due to a real effect. This is the art of statistics."
  },
  {
    "objectID": "introduction.html#the-discipline-of-statistics",
    "href": "introduction.html#the-discipline-of-statistics",
    "title": "1  Introduction",
    "section": "1.2 The discipline of statistics",
    "text": "1.2 The discipline of statistics\nThe discipline of statistics includes two main branches:\n\ndescriptive statistics\ninferential statistics\n\n\n\n\n\n\nflowchart LR\n  \n    A[Statistics]--> B[Descriptive statistics]\n    A --> C[Inferential statistics]\n    B --> D[Measures of location:  e.g., mean, median, mode.]\n    B --> E[Measures of dispersion:  e.g., standard deviation, quartiles]\n    B --> F[Measures of association:  e.g., correlation coefficients]\n    C --> G[Estimation]\n    C --> H[Hypothesis Testing]\n\n\n\n\n\nFigure 1.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics\n\n\n\n\n\nDescriptive Statistics\nThe methods and techniques of descriptive statistics aim at summarizing large quantities of data by a few numbers, in a way that highlights the most important numerical features of the data.\nDescriptive statistics include measures of central tendency, measures of dispersion, measures of position, and measures of association. They also include a description of the general shape of the distribution of the data. These terms will be explained in the corresponding chapters.\n\n\nInferential Statistics\nInferential statistics aim at generalizing a measure taken on a small number of cases that have been observed, to a larger set of cases that have not been observed. Using the terms explained above, we could reformulate this aim, and say that inferential statistics aim at generalizing observations made on a sample to a whole population."
  },
  {
    "objectID": "introduction.html#why-jamovi",
    "href": "introduction.html#why-jamovi",
    "title": "1  Introduction",
    "section": "1.3 Why Jamovi?",
    "text": "1.3 Why Jamovi?\nJamovi is a new fee open “3rd generation” statistical software that is built on top of the programming language R (Figure 1.2). Designed from the ground up to be easy to use, Jamovi is a compelling alternative to costly statistical products such as SPSS and SAS.\n\n\n\nFigure 1.2: Jamovi is free and open statistical software\n\n\n\n\n\n\n\n\nSome other advantages are:\n\n\n\n\nEnables integration with R\nIt provides informative tables and neat visuals\nGives access to a user guide and community resources from the Jamovi website"
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "\n2  LAB I: Introduction to Jamovi\n",
    "section": "",
    "text": "The goal of this lab is to introduce you to jamovi, which we’ll be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions."
  },
  {
    "objectID": "lab1.html#downloading-and-installing-jamovi",
    "href": "lab1.html#downloading-and-installing-jamovi",
    "title": "\n2  LAB I: Introduction to Jamovi\n",
    "section": "\n2.1 Downloading and installing Jamovi",
    "text": "2.1 Downloading and installing Jamovi\nJamovi is available for Windows (64-bit), macOS, Linux and ChromeOS. Installation on desktop is quite straight-forward. Just go to the Jamovi download page https://www.jamovi.org/download.html, and download the latest version (current release) for your operating system."
  },
  {
    "objectID": "lab1.html#navigating-jamovi",
    "href": "lab1.html#navigating-jamovi",
    "title": "\n2  LAB I: Introduction to Jamovi\n",
    "section": "\n2.2 Navigating Jamovi",
    "text": "2.2 Navigating Jamovi\nWhen first starting jamovi, we will be presented with a user interface which looks something like this Figure 2.1.\n\n\n\n\nFigure 2.1: The menu bar provides access to all functions of the program.\n\n\n\n\nTo the left is the spreadsheet view, and to the right is where the results of statistical tests appear. Down the middle is a bar separating these two regions, and this can be dragged to the left or the right to change their sizes.\n \nLet’s take a quick look at the Jamovi Main Menu, referred to hereafter as the Menu, as shown in (Figure 2.2). This Menu is displayed at the very top of the Jamovi screen:\n\n\n\n\nFigure 2.2: The menu bar provides access to all functions of the program.\n\n\n\n\nThere are six tabs in the Menu (from left to right): 1. File (a layer with three horizontal levels \\(\\equiv\\)), 2. Variables, 3. Data, 4. Analyses, 5. Edit, 6. Settings (the three dots \\(\\vdots\\) at the top right of the window) tabs (Table 2.1).\n\n\nTable 2.1: Menu and toolbars of Jamovi\n\n\n\n\n\nMenu tab\nToolbar\n\n\n\n\n\nFile tab (\\(\\equiv\\))\nThe file tab  allows us to open/import existing files, save and export our files.\n\n\n\n\n\n\n\nVariables tab\nThis allows us to view and search our variables in a list view.\n\n\n\n\nThis view allows us to easily navigate our variables and do the following:\n\nSearch for a variable by scrolling through the list or search for one by name.\nEdit the variable names and descriptions by double-clicking in the relevant field.\nEdit our variable details by double-clicking on the data symbol - the screen will appear for us to add all the necessary information.\nCreate a new variable by clicking on the + in the bottom right corner.\n\n\n\n\n\n\nData tab\nHere we will see our raw data which are organised like Excel in rows and columns. We can also manipulate our data and add new variables when necessary.\n\n\n\n\nSpecifically, this tab allows us to do the following:\n\nRename and add details to existing variables. Click on the Setup button, or double-click on the variable we want to manage.\nCompute and transform variables\nAdd and/or Delete variables (columns)\nAdd Filters\nAdd and/or Delete Rows\n\n\n\n\n\n\nAnalyses tab\nIt includes the available statistical analyses that can be performed by Jamovi.\n\n\n\n\nWe will spend most of our time in the Analyses Tab. The following six modules are pre-installed:\n\nExploration\nT-Tests\nANOVA\nRegression\nFrequencies\nFactor\n\nFor example, if we want to perform regression analysis, we simply click the ’’Regression” button.\nAll other modules need to be installed using the Modules button (Plus button) in your top-right \n\n\n\n\n\nEdit tab\nIt includes a toolbar similar to a word processor.\n\n\n\n\nWe can add extra information to our results using the buttons that are very similar to what we would find in Word (though there are fewer options).\n\n\n\n\n\nSettings tab\n(the three dots \\({\\vdots}\\) at the top right of the window)\n\nIt includes the application settings that can be manged by the users according to their preferences.\n\n\n\n\nWe can apply our preferences for a number of settings such as:\n\nHow many decimal numbers we want.\nIf we want to learn R, we can also display the R syntax.\nOur graph colour scheme\nOur default missing value."
  },
  {
    "objectID": "lab1.html#importing-exporting-saving-your-data",
    "href": "lab1.html#importing-exporting-saving-your-data",
    "title": "\n2  LAB I: Introduction to Jamovi\n",
    "section": "\n2.3 Importing, Exporting & Saving your Data",
    "text": "2.3 Importing, Exporting & Saving your Data\nIt is possible to simply begin typing values into the Jamovi spreadsheet as we would with any other spreadsheet software. Alternatively, existing data sets in a range of formats (CSV, Excel, SPSS, R data, Stata, SAS) can be opened in Jamovi.\nTo open a file, select the File tab  at the top left hand corner, select ‘Open’ and then ‘This PC’, and choose from the files listed on ‘Browse’ that are stored on our computer:\n\n\n\n\n\nflowchart LR\n  A[File tab ] -.-> B(Open) -.-> C(This PC) -.-> D(Browse)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Open an existing file stored on our computer into Jamovi."
  },
  {
    "objectID": "descriptive.html",
    "href": "descriptive.html",
    "title": "3  Descriptive statistics",
    "section": "",
    "text": "A variable is a quantity or property that is free to vary, or take on different values. Researchers design experiments to test if changes to one or more variables are associated with changes to another variable of interest. For example, if a researcher thinks a new treatment reduces the number of depressive symptoms, he could design an experiment to test this hypothesis; participants should randomly assigned to one of two groups: one (the experimental group) receiving the new treatment that is being tested, and the other (the comparison group or control) receiving an alternative (conventional) treatment.\nIn this experiment, the type of treatment each participant received (i.e., new treatment vs. no treatment) is the independent variable (IV), while the number of depressive symptoms observed in each person is the dependent variable (DV) or the outcome variable."
  },
  {
    "objectID": "descriptive.html#types-of-data",
    "href": "descriptive.html#types-of-data",
    "title": "3  Descriptive statistics",
    "section": "\n3.2 Types of Data",
    "text": "3.2 Types of Data\nData in variable can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (Figure 3.1).\n\n\n\n\nFigure 3.1: Broad classification of the different types of data with examples.\n\n\n\n\n\n\n\n\n\n\nExample from the literature - Salicylic acid plasters for treatment of foot corns\n\n\n\nIn a randomized controlled trial (RCT), Farndon et al. (2013) investigated the effectiveness of salicylic acid plasters compared with usual scalpel debridement for treatment of foot corns. The recorded data is as follows (Figure 3.2):\n\n\n\nFigure 3.2: The recorded data in Farndon et al. (2013) study.\n\n\n\n \nTable 3.1 presents a typical table with some of the basic characteristics (variables) of patients entered into Farndon et al. (2013) study.\n\n\nTable 3.1: Baseline characteristics of patients with foot corns by treatment group. (Farndon et al. 2013)\n\n\n\n\n\n\n\n\n\n\n\nCorn Plaster, n (%)\n(n=101)\n\nScalpel, n (%)\n(n=101)\n\n\n\nGender\nMale\n42 (42)\n42 (42)\n\n\n\nFemale\n59 (59)\n59 (58)\n\n\nCenter\nCentral\n58 (57)\n52 (52)\n\n\n\nManor\n13 (13)\n20 (20)\n\n\n\nJordanthorpe\n10 (10)\n14 (14)\n\n\n\nLimbrick\n3 (3)\n6 (6)\n\n\n\nFirth Park\n7 (7)\n4 (4)\n\n\n\nHuddersfield\n5 (5)\n4 (4)\n\n\n\nDarnall\n5 (5)\n1 (1)\n\n\n\nSmoking\nHistory\n\nNon-smoker\n34 (35)\n40 (40)\n\n\n\nPrevious smoker\n22 (22)\n16 (16)\n\n\n\nCurrent smoker\n42 (43)\n43 (43)\n\n\n\nMissing\n3 (3)\n2 (2)\n\n\nNumbers of corns\n1\n48 (48)\n66 (65)\n\n\n\n2\n28 (28)\n23 (23)\n\n\n\n3\n24 (24)\n12 (12)\n\n\n\nMissing\n1 (1)\n0 (0)\n\n\n\nAge (yrs),\nmean (sd)\n\n\n58.5 (15.6)\n59.7 (17.5)\n\n\n\nCorn size (mm),\nmedian (IQR)\n\n\n4 (3, 5)\n3 (3, 5)\n\n\n\nEQ-5D,\nmedian (IQR)\n\n\n0.73 (0.59, 0.80)\n0.73 (0.66, 0.80)\n\n\n\n\nIn this example since we have 101 patients in each randomized group the percentages are almost the same as the raw counts. However, for most studies we are unlikely to have exactly 100 participants in each group!\n\n\n \nCategorical Data\nA. Nominal Data\nNominal categorical data are data that one can name and put into categories. They are not measured but simply counted. They often consist of unordered ‘either–or’ type observations which have two categories and are often know as binary. For example: dead or alive; male or female; cured or not cured; pregnant or not pregnant. In Table 3.1 gender is a binary variable. However, nominal categorical data often can have more than two categories, for example, blood group A, B, AB, 0; country of origin; ethnic group; eye color. The Table 3.1 gives the number and percentages of people treated at each of the seven centers in each of the two randomized groups.\n\n\n\n\n\n\nWarning\n\n\n\nNumerical representation of categories are just codes\nWe can denote a male and female as 1 and 2 for gender and denote A, B, AB and 0, as 1, 2, 3, and 4 for blood type. Unlike numerical data, the numbers representing different categories do not have mathematical meanings (they are just codes).\n\n\n \nΒ. Ordinal Data\nIf there are more than two categories of classification it may be possible to order them in some way. For example, after treatment a patient may be either improved, the same or worse. Another example of an ordinal variable is the variable pain where a subject is asked to describe their pain verbally as minimal, moderate, severe, or unbearable. In Table 3.1 smoking history is given in three categories: non-smoker, previous smoker, and current smoker. Thus, someone who is a current smoker has more recent exposure to tobacco than someone who is an ex-smoker and someone who has never smoked. However, without further knowledge (of the current and past levels of tobacco consumption) it would be wrong to ascribe a numerical quantity to the category, for example, non-smoker=0, previous smoker=1, and current smoker=2, as one cannot say that someone who is current smoker has twice the levels of tobacco consumption as someone who is a previous smoker.\n\n\n\n\n\n\nWarning\n\n\n\nCollapsion of categories leads to a loss of information\nOrdinal data are often reduced to two categories to simplify analysis and presentation, which may result in a considerable loss of information.\n\n\n \nNumerical Data\nA. Count (or discrete) Data\nTable 3.1 gives details of the number of corns each participant had at the start of the trial, since this can only be a whole number or integer value, for example, 0, 1, 2, or 3 in this trial, this is termed count data. Other examples are often counts per unit of time such as the number of deaths in a hospital per year, the number of visits to the GP in a year, or the number of attacks of asthma a person has per month. In dentistry, a common measure is the number of decayed, filled or missing teeth (DFM).\nThe difference between such data as these and the ordered categorical data described earlier can be seen by considering an example of each:\n\n\n\n\n\n\nIllustrative Example: Ordinal Vs Discrete data\n\n\n\nOrdinal categorical: Stage of breast cancer: I II III IV\nDiscrete numerical: Number of children: 0 1 2 3 4 5+\nWe cannot say that stage IV is twice as bad as stage II nor that the difference between stages I and II is equivalent to that between stages III and IV. In contrast, three children are three times as many as one, and a difference of one means the same throughout the range of values.\n\n\n \n\n\n\n\n\n\nWarning\n\n\n\nDiscrete Vs Ordinal Data\nIn practice discrete data are often treated in statistical analyses as if they were ordered categories. This is not wrong, but it may not be getting the most out of the data. Conversely, when ordered categories are numbered, as with stage of disease, the temptation to treat these numbers as statistically meaningful must be resisted. For example, it is not sensible to calculate the average stage of cancer. The only information the numbers contain is in the ordering, which would be conveyed equally by calling them A, B, C, D and so on.\n\n\n \nΒ. Continuous (or measured) Data\nSuch data are measurements that can, in theory at least, take any value within a given range (they are restricted by the accuracy of the measuring instrument). These data contain the most information, and are the ones most commonly used in statistics. Examples of continuous data in Table 3.1 are: age, corn size, and EQ-5D.\nSometimes it is reasonable to treat discrete data as if they were continuous, at least as far as statistical analysis goes. While age is a continuous measurement, age at last birthday is discrete. In studies of adults with ages ranging from, say, 16 to 80, no harm is done in considering age in years as a continuous measurement (and this is standard practice), but for studies of pre-school children it would be better to use age in months. Heart rate (in beats per minute) is another discrete measurement that is usually regarded as continuous. Although the essential requirement for this change of status is that there should be a large number of different possible values, in practice we do not worry too much about analysing discrete measurements as if they were continuous.\n\n\n\n\n\n\nWarning\n\n\n\nCategorization of numerical data leads to a loss of information\nFor simplicity, it is often the case in medicine that continuous data are dichotomized to make nominal data. For example, the diastolic blood pressure (DBP), which is continuous, is converted into hypertension (>90 mmHg) and normotension (≤90 mmHg). There are two main reasons for doing this. It is easier to describe a population by the proportion of people affected, for example, the proportion of people in the population with hypertension is 10%. Further, one often has to make a decision: if a person has hypertension, then they will get treatment, and this is easier if high blood pressure has been categorized. However, this dichotomization clearly leads to a loss of information.\n \nOne can also divide a continuous variable into more than two groups. For example, we could divide age into age bands of equal lengths of, say 10 years such as: 0-9, 10-19, 20-29, etc. When categorizing continuous data authors should give an indication as to why they chose these cut-off points, and a reader has to be very wary to guard against the fact that the cuts may be chosen to make a particular point.\nSome statisticians have termed the habit of categorizing continuous variables as “dichotomania”, which they regard as poor practice since it loses information and assumes a discontinuous relationship that is unlikely in nature.\n\n\n \n\n\n\n\n\n\nTip\n\n\n\nRecord the actual values\nIt is best to record the actual value of blood pressure, haemoglobin, etc. It is easy to convert to categories in the analysis, but the raw data cannot be retrieved later if only categories are recorded. Information is lost with no compensatory gain. Indeed, the statistical analysis of continuous data is more powerful, and often simpler.\n \nWhen some calculation is necessary to derive the observation of interest this should be done by the computer. Thus it is much better to record date of birth and date of examination for subsequent calculation of age rather than to rely on mental arithmetic.\n\n\nThe degree of measurement accuracy and the type of data are both important in relation to carrying out a proper statistical analysis.\n  ## Summarizing Categorical Data\nBinary data are the simplest type of data. Each individual has a label which takes one of two values such as male or female, corn healed or not healed. A simple summary would be to count the different types of labels and find the frequencies. The set of frequencies of all the possible categories is called the frequency distribution of the variable.\nHowever, a raw count is rarely useful. For example, in Table 3.1 there are more non-smokers in the scalpel group (40 out of 99 or 40%) compared to corn plaster group (34 out of 98 or 35%). It is only when this count is expressed as a proportion (relative frequency) that it becomes useful. Hence the first step to analyzing categorical data is to count the number of observations in each category (frequencies) and express them as proportions of the total sample size (relative frequencies).\n\n\n\n\n\n\nIllustrative Example - Salicylic acid plasters for treatment of foot corns\n\n\n\nFarndon et al. (2013) reports a RCT that investigated the effectiveness of salicylic acid plasters compared with usual scalpel debridement for treatment of foot corns. As we have already mentioned one categorical variable recorded was the center where each trial participant was treated. Trial participants were treated at one of seven centers and the corresponding categories as displayed in Table 3.2. The first column shows category (treatment center) names, whilst the second shows the number of individuals in each category together with its percentage contribution to the total. Table 3.2 clearly shows that the majority (54.5%) of patients were treated at the “Central” treatment center.\n\n\nTable 3.2: Frequency and percentage distributions of treatment center among 202 patients with corns (Farndon et al. 2013)\n\n\nTreatment center\nFrequency\nPercentage\n\n\n\nCentral\n110\n54.5%\n\n\nManor\n33\n16.3%\n\n\nJordanthorpe\n24\n11.9%\n\n\nLimbrick\n9\n4.5%\n\n\nFirth Park\n11\n5.4%\n\n\nHuddersfield\n9\n4.5%\n\n\nDarnall\n6\n3.0%\n\n\nTotal\n202\n100.0%\n\n\n\n\nIn addition to tabulating each variable separately, we might be interested in whether the distribution of patients across each center is the same for each randomized group. Table 3.3 shows the distribution of the number of patients treated at center by randomized group; in this case it can be said that the treatment center has been cross-tabulated with randomized group. Table 3.3 is an example of a contingency table with seven rows (representing treatment center) and two columns (randomized group). Note that we are interested in the distribution of patients across the seven centers in each randomized group (to see whether or not we have similar numbers of patients randomized to each treatment within each center), and so the percentages add to 100 down each column, rather than across the rows.\n\n\nTable 3.3: Cross-tabulation distribution of treatment center by randomized group for 202 patients with corns (Farndon et al. 2013)\n\n\n\nCorn plaster, n(%)\nScalpel, n(%)\nAll, n(%)\n\n\n\n\n\n\n\n\n\nCentral\n58 (57)\n52 (52)\n110 (54.5)\n\n\nManor\n13 (13)\n20 (20)\n33 (16.3)\n\n\nJordanthorpe\n10 (10)\n14 (14)\n24 (11.9)\n\n\nLimbrick\n3 (3)\n6 (6)\n9 (4.5)\n\n\nFirth Park\n7 (7)\n4 (4)\n11 (5.4)\n\n\nHuddersfield\n5 (5)\n4 (4)\n9 (4.5)\n\n\nDarnall\n5 (5)\n1 (1)\n6 (3.0)\n\n\nTotal\n101 (100)\n101 (100)\n202 (100)\n\n\n\n\n\n\n \n\n\n\n\n\n\nTip\n\n\n\nRecommendations for reporting numbers\n\n\nTable 3.4: Reporting numbers and percentages\n\n\n\n\n\nRecommendation\nCorrect expression\n\n\n\nNumbers\n\n\n\nIn a sentence, numbers less than 10 are words.\nSmoking history was missing from three patients in the corn plaster study group.\n\n\n\n\n\n\nIn a sentence, numbers 10 or more are numbers.\nThere are 34 non-smokers patients in the corn plaster group.\n\n\n\n\n\n\nUse words to express any number that begins a sentence, title or heading.\n\nThirty-four non-smokers patients recorded in the cord plaster group.\n\n\n\n\n\n\nPercentages\n\n\n\nReport percentages to only one decimal place if the sample size is larger than 100.\nIn the sample of 202 patients, 4.5% were treated at the “Limbrick” treatment center.\n\n\n\n\n\n\nReport percentages with no decimal places if the sample size is less than 100.\nIn the sample of 98 patients in the corn plaster group, 35% were non-smokers.\n\n\n\n\n\n\nDo not use percentages if the sample size is less than 20.\nFrom 16 previous smokers in the scalpel group, 7 were females."
  },
  {
    "objectID": "descriptive.html#displaying-categorical-data",
    "href": "descriptive.html#displaying-categorical-data",
    "title": "3  Descriptive statistics",
    "section": "\n3.3 Displaying Categorical Data",
    "text": "3.3 Displaying Categorical Data\nWhile frequency tables are extremely useful, the best way to investigate a data set is to plot it. For categorical variables, such as gender and treatment center, it is straightforward to present the number in each category, usually indicating the frequency and percentage of the total number of patients. When shown graphically this is called a bar plot.\n \nA. Simple Bar Plot\nA simple bar plot is an easy way to make comparisons across categories. Figure 3.3 shows the centers where 202 patients with foot corns were treated in the trial of Farndon et al. (2013). Along the horizontal axis (x-axis) are the different treatment centers whilst on the vertical axis (y-axis) is the percentage. The height of each bar represents the percentage of the total patients in that category. For example, it can be seen that the percentage of participants who were treated in the “Central” center was about 55%.\n\n\n\n\nFigure 3.3: Bar plot showing where 202 patients with corns were treated (Farndon et al. 2013).\n\n\n\n\n\n\n\n\n\n\nBasic Properties of Simple Bar plot\n\n\n\n\nAll rectangular bars should have equal width and should have equal space between them.\nThe rectangular bars can be drawn horizontally or vertically.\nThe height of the rectangular bar is equivalent to the data they represent.\nThe rectangular bars must be plotted against a common zero-valued baseline.\n\n\n\n \nB. Side-by-side and Grouped Bar Plots\nIf the sample is further classified into whether the patient was treated with corn plasters or scalpel then it becomes impossible to present the data as a single bar plot. We could present the data as a side by side bar plot (see Figure 3.4) but is preferable to present the data in one graph with the same scales and axes to make the visual comparisons easier (grouped bar plot) (see Figure 3.5).\n\n\n\n\nFigure 3.4: Side-by-side bar plot showing where 202 patients with corns were treated by randomized group (Farndon et al. 2013).\n\n\n\n\n\n\n\n\nFigure 3.5: Grouped bar plot showing where 202 patients with corns were treated by randomized group (Farndon et al. 2013).\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nReport the actual total sample sizes for each group\nIf we do use the relative frequency scale as we have, then it is recommended to report the actual total sample sizes for each group (e.g., in the legend or caption). In this way, given the total sample size and relative frequency (from the height of the bars) we can work out the actual numbers treated in each center.\n\n\n \nC. Stacked Bar Plot\nUnlike a side-by-side or grouped graphs, Stacked Bar Plots segment their bars. A 100% Stack Bar Plot shows the percentage-of-the-whole of each group and are plotted by the percentage of each value to the total amount in each group. This makes it easier to see if relative differences exist between quantities in each group (see Figure 3.6).\n\n\n\n\nFigure 3.6: A horizontal 100% stacked bar plot showing the distribution of gender by randomized group (Farndon et al. 2013).\n\n\n\n\nIn Figure 3.6 the bars are divided into two segments only (i.e., female and male) so it is easy to read the values of each segment and to compare a specific segment through the entire set of bars (in our case the percentages are equal). This comparison can be easily made because each segment is aligned through the entire set of bars (female to the left and men to the right). If more segments were added, however, the segments in the middle would not be aligned to the left or right, which would make comparisons difficult (see Figure 3.7).\n\n\n\n\nFigure 3.7: A horizontal 100% stacked bar plot showing the distribution of treatment centers by randomized group (Farndon et al. 2013).\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStacked bar plots tend to become confusing when the variable has many levels\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many categories, stacked bar plots tend to become rather confusing."
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data",
    "href": "descriptive.html#summarizing-numerical-data",
    "title": "3  Descriptive statistics",
    "section": "\n3.4 Summarizing Numerical Data",
    "text": "3.4 Summarizing Numerical Data\nA quantitative measurement contains more information than a categorical one, and so summarizing these data is more complex. One chooses summary statistics to condense a large amount of information into a few intelligible numbers, the sort that could be communicated verbally. The two most important pieces of information about a quantitative measurement are ‘where is it?’ and ‘how variable is it?’ These are categorized as measures of location (or sometimes ‘central tendency’) and measures of spread or variability.\n\n\n\n\n\n\nTwo summary measures should be reported for a numerical variable\n\n\n\nA measure of location (where the center of the distribution of the values is located) and variability (how widely the values are spread above and below the central value) provides an informative but brief summary of a set of observations.\n\n\n \nMeasures of Location\nA. Sample Mean or Average\nLet \\(x_1, x_2,...,x_{n-1}, x_n\\) be a set of n measurements. The arithmetic sample mean or average, \\(\\bar{x}\\) (pronounced x bar), is simply the sum of the observations divided by their number n, thus:\n\\[\n\\bar{x}= \\frac{Sum \\ of \\ all \\ sample \\ values }{Size \\ of \\ sample}= \\frac{x_1 + x_2 + ... + x_{n-1} + x_n}{n}\n\\]\nThis formula is entirely correct, but it’s too long, so we make use of the summation symbol \\(\\scriptstyle\\sum\\) to shorten it:\n\\[\\bar{x}=\\frac{\\sum_{i=1}^{n}x_{i}}{n}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i} \\tag{3.1}\\]\nIn the above Equation 3.1, \\(x_{i}\\) represents the individual sample values and \\({\\sum_{i=1}^{n}x_{i}}\\) their sum. The Greek letter \\({\\Sigma}\\) (sigma) is the Greek capital ‘S’ and stands for ‘sum’ and simply means ‘add up the n observations \\(x_{i}\\) from the 1st to the last (nth)’.\nUsually, we cannot measure the population mean \\({\\mu}\\), which is the unknown constant that we want to estimate using the sample mean \\(\\bar{x}\\).\n \n\n\n\n\n\n\nExample: Calculation of the Mean - Corn size data (mm)\n\n\n\nIn the RCT by Farndon et al. (2013), the baseline size of the corn (as its widest diameter in mm) was measured by a podiatrist (foot specialist). Consider the following 16 baseline corn sizes selected from the patients:\n2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3\n \nThe sum of the 16 observations is:\n\\(2 + 2 + 6 + 3 + 4 + 2 + 2 + 5 + 3 + 4 + 1 + 2 + 6 + 3 + 10 + 3\\) = 58\n \nThus, the arithmetic mean is:\n\\(\\bar{x}\\) = 58/16 = 3.625 mm or 3.6 mm. It is usual to report one more decimal place for the mean than the data recorded.\n\n\nThe major advantage of the mean is that it uses all the data values, while the main disadvantage is its sensitivity to very large or very small values, which might be outliers (unusual values). For example, if we entered “100 mm” instead of “10 mm”, for the 15th patient, in the calculation of the mean, we would find the mean changed from 3.6 to 9.2. It does not necessarily follow, however, that outliers should be excluded from the final data summary, or that they result from a human error. Outliers can be legitimate anomalies that are vital for capturing information on the subject of interest.\nIf the data are binary and are coded 0 or 1, then \\(\\bar{x}\\) is the proportion of individuals with value 1, and this can also be expressed as a percentage. In Farndon et al. (2013) data, the cases in which the corn was healed are coded as ‘1s’ and the cases in which the corn was not healed as ‘0s’. The corn had healed in 52 out of 189 patients, so the mean of this variable is 0.275 or 28%.\n\n\n\n\n\n\nAdvantages and Disadvantages of arithmetic mean\n\n\n\nAdvantages of mean\n\nis simple to understand and easy to calculate\nuses all the data values in the calculation\nis algebraically defined and thus mathematically manageable\nhas a known sampling distribution\n\n \nDisadvantages of mean\n\nis highly affected by the presence of a few abnormally high or abnormally low values (outliers)\nis not an appropriate average for highly skewed (asymmetrical) distributions\ncannot be determined if any item of observation is missing\ncannot be determined easily by inspection of the data\n\n\n\n \nB. Median\nThe sample median, md, is an alternative measure of location, which is less sensitive to outliers. For observed values \\(x_1, x_2,...,x_{n-1}, x_n\\) the median is calculated by first sorting the observed values (i.e., arranging them in an ascending/descending order) and selecting the middle one. If the sample size n is odd, the median is the number at the middle of the ordered observations. If the sample size is even, the median is the average of the two middle numbers.\nTherefore, the sample median, md, of n observations is:\n\nthe \\(\\frac{n+1}{2}\\)th ordered value, \\(md=x_{\\frac{n+1}{2}}\\), if n is odd.\nthe average of the \\(\\frac{n}{2}\\)th and \\(\\frac{n+1}{2}\\)th ordered values, \\(md=\\frac{1}{2}(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}})\\), if n is even.\n\n\n\n\n\n\n\nExample: Calculation of the Median - Corn size data (mm)\n\n\n\n1st case: Even observations\n\nWe have selected 16 baseline corn sizes:\noriginal data: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3\nWe arrange the data in an ascending order:\nordered data: 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 10\nAs the number of observations are even (n=16), the median is the average of the two middle ordered numbers (the eighth and ninth): (3+3)/2=3 mm.\n\n \n2nd case: Odd observations\n\nSuppose we select an additional 17th subject with corn size of 10 mm, so the data are as following:\noriginal data: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3, 10\nWe arrange the data in an ascending order:\nordered data: 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 10, 10\nThe median would be the 9th ordered observation, which is also 3 mm.\n\n \nMain Advantages and Disadvantages of median\nThe median has the advantage that is not affected by outliers, so for example, the median in the data would be unaffected by replacing the largest corn size of 10 mm with 100 mm. However, it does not take into account the precise value of each observation and hence does not use all information available in the data.Additionally, it is not a good measure of central tendency when there are heavy ties in the data.\n\n\n \nC. Mode\nA third measure of location is termed the mode. This is the value that occurs most frequently, or, if the data are grouped, the group with the highest frequency. It is not used much in statistical analysis, since its value depends on the accuracy with which the data are measured and ignores most of the information; although it may be useful for categorical data to describe the most frequent category. Note that some data sets do not have a mode because each value occurs only once.\nHowever, the expression ‘bimodal’ distribution is used to describe a distribution with two peaks in it. This can be caused by mixing two or more populations together. For example, height might appear to have a bimodal distribution if one had men and women in the population.\n\n\n\n\n\n\nExample: Calculation of the Mode - Corn size data (mm)\n\n\n\noriginal data: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3\nIn the 16 patients with corns, 5 patients have corn size of 2 mm; thus, the modal corn size is 2 mm.\n\n\n \nMeasures of Dispersion\nWe also need a numerical way of summarizing the amount of spread or variability in a data set. The tree main approaches to quantifying variability are: the range, the interquartile range (IQR), and the standard deviation.\nA. Range\nThe simplest way to describe the spread of a data set is to report the minimum (lowest) and maximum (highest) values. The range is defined as the difference between the largest and the smallest observations in a sample. For some data it is very useful, because one would want to know these numbers, for example in a sample the age of the youngest and oldest participant. However, if outliers are present it may give a distorted impression of the variability of the data, since only two of the data points are included in making the estimate. Thus the range is affected by extreme values at each end of the data.\n\n\n\n\n\n\nExample: Calculation of the Range - Corn size data (mm)\n\n\n\nThe range for the corn size data is 1 to 10 mm or described by a single number 10-1 = 9 mm.\n\n\n \nB. Quartiles and Interquartile Range\nThe quartiles, namely the lower quartile (\\(Q_{1}\\)), the median (\\(Q_{2}\\)) and the upper quartile (\\(Q_{3}\\)), split sorted data into four equal parts. That is there will be approximately equal numbers of observations in the four sections (and exactly equal if the sample size is divisible by four and the measures are all distinct). The quartiles are calculated in a similar way to the median; first order the data and then count the appropriate number from the bottom. The \\(Q_{1}\\) is the value below which 25% of the observations may be found, while the \\(Q_{3}\\) is the value above which the top 25% of the observations may be found (meaning that 75% of the data falls below the \\(Q_{3}\\)).\nThe interquartile range (IQR) is a useful measure of variability and is given by the difference of the lower and upper quartiles (IQR=\\(Q_{3}\\)-\\(Q_{1}\\)). It indicates the spread of the middle 50% (75%-25%) of the data.The IQR is an especially good measure of variability for skewed distributions or distributions with outliers.\nThe median and the quartiles are examples of percentiles - points which split the distribution of data into percentages above or below a certain value. The median is the 50th percentile, the \\(Q_{1}\\) is the 25th percentile, and the the \\(Q_{3}\\) is the 75th percentile.\n\n\n\n\n\n\nExample: Calculation of the Quartiles and Interquartile Range - Corn size data (mm)\n\n\n\nThe \\(Q_{1}\\) lies somewhere between the \\(\\color{red}{fourth}\\) and \\(\\color{blue}{fifth}\\) ordered observations (2+2)/2= 2mm. The median, \\(Q_{2}\\), is the average of the \\(\\color{blue}{eighth}\\) and \\(\\color{green}{ninth}\\) ordered observations (3+3)/2=3 mm. Similarly, The \\(Q_{3}\\) lies somewhere between the \\(\\color{green}{12th}\\) and \\(\\color{orange}{13th}\\) ordered observations (4+5)/2= 4.5 mm.\nSo, the IQR for the corn size data is from 2.0 to 4.5 mm; or a single number 2.5 mm.\n\n\nTable 3.5: Calculating quartiles for the corn size data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrdered corn size (mm)\n\\(\\color{red}{1}\\)\n\\(\\color{red}{2}\\)\n\\(\\color{red}{2}\\)\n\\(\\color{red}{\\textbf{2}}\\)\n\\(\\color{blue}{\\textbf{2}}\\)\n\\(\\color{blue}{2}\\)\n\\(\\color{blue}{3}\\)\n\\(\\color{blue}{\\textbf{3}}\\)\n\\(\\color{green}{\\textbf{3}}\\)\n\\(\\color{green}{3}\\)\n\\(\\color{green}{4}\\)\n\\(\\color{green}{\\textbf{4}}\\)\n\\(\\color{orange}{\\textbf{5}}\\)\n\\(\\color{orange}{6}\\)\n\\(\\color{orange}{6}\\)\n\\(\\color{orange}{10}\\)\n\n\n\nThe range for the corn size data is 1 to 10 mm or described by a single number 10-1 = 9 mm.\n\n\n \nC. Variance and Standard Deviation\nFor an individual with an observed value \\(x_{i}\\) the distance from the mean is \\(x_{i}-\\bar{x}\\). With n such observations we have a set of n differences, one for each individual. The sum of the differences, \\({\\sum_{i=1}^{n}(x_{i}-\\bar{x})}\\) is always zero. However, if we square the distances before we sum them, we get always a positive quantity.This sum is then divided by n-1 and thus gives an average measure for the square of the deviation from the sample mean. This quantity is called the sample variance and is defined as Equation 3.2:\n\\[variance = s^2 = \\frac{\\sum\\limits_{i=1}^n (x -\\bar{x})^2}{n-1} \\tag{3.2}\\]\nThe variance is expressed in square units, so we can take the square root to return to the original units. This gives us the standard deviation (usually abbreviated as sd) defined as Equation 3.3:\n\\[sd=s = \\sqrt\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1} \\tag{3.3}\\]\nExamining this expression it can be seen that if all the x’s were the same, then they would equal x and so sd would be zero. If the x’s were widely scattered about x, then sd would be large. In this way sd reflects the variability in the data. Both, variance and standard deviation, are sensitive to outliers and thus they are inappropriate for skewed data.\n\n\n\n\n\n\nExample: Calculation of the Variance and Standard Deviation - Corn size data (mm)\n\n\n\nConsider the 16 observations from the corn data. The calculations to work out the standard deviation are given in the Table 3.6.\n\n\nTable 3.6: Calculating variance and standard deviation for the corn size data\n\n\n\n\n\n\n\n\nid\nCorn size (mm)\nMean\nDifference from mean \\((x_{i}-\\bar{x})\\)\n\nSquare of difference from mean \\((x_{i}-\\bar{x})^2\\)\n\n\n\n\n1\n2\n3.625\n-1.625\n2.641\n\n\n2\n2\n3.625\n-1.625\n2.641\n\n\n3\n6\n3.625\n2.375\n5.641\n\n\n4\n3\n3.625\n-0.625\n0.391\n\n\n5\n4\n3.625\n0.375\n0.141\n\n\n6\n2\n3.625\n-1.625\n2.641\n\n\n7\n2\n3.625\n-1.625\n2.641\n\n\n8\n5\n3.625\n1.375\n1.891\n\n\n9\n3\n3.625\n-0.625\n0.391\n\n\n10\n4\n3.625\n0.375\n0.141\n\n\n11\n1\n3.625\n-2.625\n6.891\n\n\n12\n2\n3.625\n-1.625\n2.641\n\n\n13\n6\n3.625\n2.375\n5.641\n\n\n14\n3\n3.625\n-0.625\n0.391\n\n\n15\n10\n3.625\n6.375\n40.641\n\n\n16\n3\n3.625\n-0.625\n0.391\n\n\nSum\n58\n3.625\n0.000\n75.756\n\n\n\n\nFrom the Equation 3.3 we have:\n\\[sd = \\sqrt\\frac{\\sum_{i=1}^{16}(x_{i}-\\bar{x})^2}{16-1} = \\sqrt\\frac{75.756}{15}= \\sqrt{5.05}=2.247 \\ or \\ 2.3 \\ mm\\]\nNote that the majority of this sum is contributed by one observation, the value of 10 mm from participant 15, which is the observation further from the mean. This shows that much of the value of an sd is derived from the outlying observation.\n \nWhy is the standard deviation useful?\n\nIt turns out in many situations that about 95% of observations will be within two standard deviations of the mean. This is known as a reference interval and it is this characteristic of the standard deviation which makes it so useful. It holds for a large number of measurements commonly made in medicine. In particular it holds for data that follow a Normal distribution (see Chapter XX). For example, if the age of participants in the corn plaster group is normally distributed, we would expect the majority of participants in this treatment group to have age between 58.5-2 \\(\\times\\) 15.6 and 58.5+2 \\(\\times\\) 15.6 or 27.3 and 89.7 years.\n\n\n\n \n\n\n\n\n\n\nTip\n\n\n\nHow to report summary statistics for numerical data?\nMean and median convey different impressions of the location of data in presence of skewness (or outliers).\n\nIf the distribution is symmetric (mean=median=mode) (Figure 3.8 b), then in general the mean is the better summary statistic (see also Chapter 5).\nIf the distribution is skewed to the left (Figure 3.8 a) or right (Figure 3.8 c) then the median is less influenced by the tails (see also Chapter 5).\n\n\n\n\n\nFigure 3.8: Types of distribution according to the summetry (a) left skewed, (b) symmetric, and (c) right skewed distribution.\n\n\n\n\nThus, the following format is recommended for reporting summary statistics for numerical data:\n\nMean (sd) for data with symmetric distribution. A distribution, or data set, is symmetric if its left and right sides are mirror images.\n\n\nMedian (Q1, Q3) for those with skewed (or asymetrical) distribution.  \n\nTable 3.7 presents recommendations for reporting summary statistics for numerical data.\n\n\nTable 3.7: Recommendations for reporting summary statistics\n\n\n\n\n\nRecommendation\nCorrect expression\n\n\n\nDo not imply greater precision than the measurement instrument\nOnly use one decimal place more than the basic unit of measurement when reporting statistics (means, medians, standard deviations, inter-quartile ranges, etc.), for example, the mean age of participants in the corn plaster group was 58.5 years.\n\n\nFor ranges use ‘to’ or a comma but not ‘-’ to avoid confusion with a minus sign. Also use the same number of decimal places as the summary statistic\n\nThe mean (sd) age of participants in the corn plaster group was 58.5 years (15.6).\nThe median (IQR) EQ-5D of participants in the corn plaster group was 0.73 (0.59, 0.80)"
  },
  {
    "objectID": "descriptive.html#displaying-numerical-data",
    "href": "descriptive.html#displaying-numerical-data",
    "title": "3  Descriptive statistics",
    "section": "\n3.5 Displaying Numerical Data",
    "text": "3.5 Displaying Numerical Data\nThe best way for examining the distribution of numerical data is to generate an appropriate graph.\nA. Histogram and density plot\nThe most common way of depicting a frequency distribution of a continuous variable is with a histogram.\nA histogram (Figure 3.9 a) is a plot that depicts the distribution of a numeric variable’s values as a series of bars without space between them. Each bar typically covers a range of numeric values called a bin or class; a bar’s height indicates the frequency of observations with a value within the corresponding bin. A density plot (Figure 3.9 b) is a smoothed, continuous version of a histogram estimated from the data. In a density plot the total area under the curve integrates to one.\nFigure 3.9 shows the distribution of age for the participants in Farndon et al. (2013) study. The vertical scale shows (a) frequency (histogram) or (b) probability density (density plot).\n\n\n\n\nFigure 3.9: Distribution of age of the 202 participants in Farndon et al. (2013) study (a) histogram (b) density plot.\n\n\n\n\nA histogram (or density plot) gives information about:\n\nHow the data are distributed: (a) left-skewed, (b) symmetric (e.g., normal distribution), (c) right-skewed and if there are any outliers.\nThe amount of variability in the data.\nWhere the peaks of the distribution are.\n\n \n\n\n\n\n\n\nChoose an appropriate number of bins\n\n\n\nWhile tools that can generate histograms usually have some default algorithms for selecting bin boundaries, we will likely want to play around with the binning parameters to choose something that is representative of our data.\nChoice of bin size has an inverse relationship with the number of bins Figure 3.10. The smaller the bin sizes, the more bins there will be to cover the whole range of data. With a larger bin size, the fewer bins there will need to be. It is worth taking some time to test out different bin sizes to see how the distribution looks in each one, then choose the plot that represents the data best.\n\n\n\n\nFigure 3.10: Histogram of age for different bin sizes (a) bin size = 2, (b) bin size = 10, and (c) bin size = 15.\n\n\n\n\nIf we have too many bins, then the data distribution will look rough, and it will be difficult to discern the signal from the noise (Figure 3.10 a). On the other hand, with too few bins, the histogram will lack the details needed to discern any useful pattern from the data (Figure 3.10 b).\n\n\nWe can also create a histogram or density plot by group. Figure 3.11 depicts the probability density of age by treatment group.\n\n\n\n\nFigure 3.11: Histogram of age of participants by treatment groupin Farndon et al. (2013) study.\n\n\n\n\n \n\n\n\n\n\n\nTip\n\n\n\nHistograms must be plotted with a zero-valued baseline\nAn important aspect of histograms is that they must be plotted with a zero-valued baseline. Since the frequency of data in each bin is implied by the height of each bar, changing the baseline or introducing a gap in the scale will skew the perception of the distribution of data.\n\n\n \nB. Box Plot\nA box plot chart is another graph that can be used for conveying location and variation information for continuous data, particularly for detecting changes between different groups of data before any formal analyses are performed.\nA box plot (aka box and whisker plot) uses boxes and lines to depict the distributions of one or more groups of numerical data. Box limits indicate the range of the central 50% of the data, with a central line marking the median value. Lines extend from each box to capture the range of the remaining data, with dots placed past the line edges to indicate outliers.\n\n\n\n\nFigure 3.12: Broad classification of the different types of data with examples\n\n\n\n\nIn Figure 3.12 the distance between \\({Q3}\\) and \\({Q1}\\) is the interquartile range (IQR) and plays a major part in how long the whiskers extending from the box are. Each whisker extends to the furthest data point in each wing that is within 1.5 times the IQR. Any data point further than that distance is considered an outlier, and is marked with a dot. There are other ways of defining the whisker lengths, which are not presented in this textbook.\nFigure 3.13 illustrates a box plot that presents the age of participants in Farndon et al. (2013) study.\n\n\n\n\nFigure 3.13: A box plot of age of the participants in Farndon et al. (2013) study.\n\n\n\n\nFigure 3.14 illustrates a grouped box plot that presents the age by treatment group.\n\n\n\n\nFigure 3.14: A box plot of age of the participants by treatment group in Farndon et al. (2013) study.\n\n\n\n\n \n\n\n\n\n\n\nIdentifying Outliers in the data based on quartiles\n\n\n\nAn outlier is a data value significantly far removed from the main body of a data set. We say any value outside of the following interval is an outlier:\n\\[(Q_1 - 1.5 \\times IQR, \\ Q_3 + 1.5 \\times IQR)\\]\n\n\n \nc. Raincloud Plot\nThere are many variations of the boxplot. For example, there is a way to combine raw data (dots), probability density, and key summary statistics such as median, and relevant intervals of a range of likely values for the population parameter, in an appealing and flexible format with minimal redundancy, using the raincloud plot (Figure 3.15):\n\n\n\n\nFigure 3.15: A raincloud plot of age of the participants in Farndon et al. (2013) study.\n\n\n\n\n \nc. Scatter plot\nScatter plots are two dimensional graphs, showing the association between two continuous variables such as age and baseline pain (Figure 3.16 a). By coloring the data by a third variable (here is gender), scatter plots can provide additional dimensions (Figure 3.16 b).\n\n\n\n\nFigure 3.16: Association between (a) age and baseline, and (b) taking into acound the gender in Farndon et al. (2013) study.\n\n\n\n\nAs the scatter plots suggest, there is not association between age and baseline pain: the points seem to be scattered randomly."
  },
  {
    "objectID": "descriptive.html#exercises",
    "href": "descriptive.html#exercises",
    "title": "3  Descriptive statistics",
    "section": "\n3.6 Exercises",
    "text": "3.6 Exercises\n\nFirst Exercise\nSecond Exercise\n\n\n\n\n\nFarndon, Lisa J, Wesley Vernon, Stephen J Walters, Simon Dixon, Mike Bradburn, Michael Concannon, and Julia Potter. 2013. “The Effectiveness of Salicylic Acid Plasters Compared with ‘Usual’ Scalpel Debridement of Corns: A Randomised Controlled Trial.” Journal of Foot and Ankle Research 6 (1): 40. https://doi.org/10.1186/1757-1146-6-40."
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "4  LAB II: Descriptive Statistics",
    "section": "",
    "text": "\\(\\equiv\\)"
  },
  {
    "objectID": "normal.html",
    "href": "normal.html",
    "title": "5  Distributions - Normal distribution",
    "section": "",
    "text": "We have mentioned the normal distribution several times in this textbook with the promise we would provide more detail later. In this section, we will explore the normal distribution, why it matters, and how we can tell if our distribution is normal or not."
  },
  {
    "objectID": "normal.html#characteristics-of-normal-distribution",
    "href": "normal.html#characteristics-of-normal-distribution",
    "title": "5  Distributions - Normal distribution",
    "section": "\n5.1 Characteristics of Normal Distribution",
    "text": "5.1 Characteristics of Normal Distribution\nHistorical background\nThere are several important probability distributions in statistics. However, the normal distribution might be the most important. A normal distribution is the familiar “bell curve” and it’s a way of formalizing a distribution where observations cluster around some central tendency. Observations farther from the central tendency occur less frequently. First, Galileo informally described a normal distribution in 1632 when discussing the random errors from observations of celestial phenomena. However, Galileo existed before the time of differential equations and derivatives. We owe its formalization to Carl Friedrich Gauss, which is why the normal distribution is often called a Gaussian distribution. A very familiar example is the height for adult people that approximates a normal distribution very well.\n\n\n\n\nFigure 5.1: A distribution can be (a) leptokurtic, (b) mesokurtic, or (c) platykurtic.\n\n\n\n\nThe mathimatical type\nGauss’ normal distribution, technically a density function, is a distribution defined by two parameters, mean \\(\\mu\\) and variance \\(\\sigma^2\\). The mean , \\(\\mu\\), is a “location parameter”, which defines the central tendency. The variance, \\(\\sigma^2\\) is the “scale parameter”, which defines the width of the distribution and how short the distribution is. It’s formally given as follows:\n\\[ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}  \\tag{5.1}\\]\nThe ensuing distribution will look like this in a simple case where \\(\\mu\\) is 0 and \\(\\sigma^2\\) is 1.\n\n\n\n\nFigure 5.2: A simple normal density function.\n\n\n\n\nIndividual components of a normal distribution\nWe can break down individual components of a normal distribution and explain them until they seem more accessible.\nFirst, the “kernel” is the part inside the exponent of the above equation (i.e. \\(\\ {-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}\\)). Observe, for a simple case where \\(\\mu\\) = 0 and \\(\\sigma^2\\) = 1 that this part becomes \\(\\ -{\\frac {1}{2}}x^{2}\\) that is a negative parabola (notice the square term). The minus sign just flips the basic parabola \\(\\ {\\frac {1}{2}}x^{2}\\) downward.\n\n\n\n\nFigure 5.3: (a) A basic parabola and (b) a negative parabola.\n\n\n\n\nSecond, exponentiating the negative parabola (\\(\\  e^{-{\\frac {1}{2}}x^{2}}\\)) makes it asymptote to 0.\n\n\n\n\nFigure 5.4: An exponentiated negative parabola.\n\n\n\n\nNotice the tails in the above graph are asymptote to 0. “Asymptote” is a fancier way of saying the tails approximate 0 but never touch or surpass 0. One way of thinking about this as we build toward its inferential implications is that deviations farther from the central tendency are increasingly “unlikely”.\nThird, and with the above point in mind, it should be clear that \\(\\ {\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}\\) will scale the height of the distribution. Observe that in our simple case where \\(\\mu\\) is 0 and \\(\\sigma^2\\) is 1, the height of the exponentiated parabola is at 1. That gets multiplied by \\(\\ {\\frac {1}{\\sqrt {2\\pi }}}\\) to equal about 0.398.\nFourth, the normal distribution is perfectly symmetrical. The mean, \\(\\mu\\) determines the location of the distribution as well as its central tendency. All three measures of central tendency, the mode (most frequently occurring value), the median (the middlemost value), and the mean (the statistical average), will be the same. It also means a given observation of x will be as far from \\(\\mu\\) as -x. Additionally, the statistical moments of skewness and excess kurtosis are zero.\nFifth, we noted the normal distribution as a function and not a probability because the probability of any one value is effectively zero.\nNormal Density Plot with Shaded Regions\nImportantly, around 68% of the distribution is between one standard unit of \\(\\mu\\). Around 90% of the distribution is between 1.645 standard units on either side of \\(\\mu\\). Around 95% of the distribution is between about 1.96 standard units on either side of \\(\\mu\\). About 99% of the distribution is between 2.58 standard units on either side of \\(\\mu\\). So, the probability that x is between 1 on either side of the \\(\\mu\\) of 0 is effectively 0.68. The ease of this interpretation is why researchers like to standardize their variables so that the mean is 0 and the standard deviation (i.e. the scale parameter) is 1.\n\n\n\n\nFigure 5.5: The area underneath a normal distribution\n\n\n\n\nThe normal distribution appears as a foundation assumption for a lot of quantitative approaches to frequentist statistics.\nIn summary, the normal density function is technically unbounded. It has just the two parameters that define its location and scale and the tails are asymptote to 0 no matter what the values of \\(\\mu\\) and \\(\\sigma^2\\) are. This makes the distribution continuous since x can range over the entire line from \\(\\ -\\infty\\) to \\(\\ +\\infty\\). Thus, the function does not reveal the probability of x (the probability of any one value is effectively 0). However, the area under the curve is the full domain of the probability space and sums to 1. The probability of selecting a number between two points on the x-axis equals the area under the curve between those two points.\n\n\n\n\nFigure 5.6: Standard normal distribution with a shaded region\n\n\n\n\nTo find the area between x = 1 and x = 2, we must integrate the Equation 5.1 as following:\n\\[ E(x)=\\int_{1}^{2}f(x)dx  \\tag{5.2}\\]"
  },
  {
    "objectID": "normal.html#properties-of-an-approximately-normal-distribution",
    "href": "normal.html#properties-of-an-approximately-normal-distribution",
    "title": "5  Distributions - Normal distribution",
    "section": "\n5.2 Properties of an approximately normal distribution",
    "text": "5.2 Properties of an approximately normal distribution\nIn an approximately bell-shaped (normal) distribution:\n\nthe mean, the median and the mode have very close values\nthe histogram is symmetric about the mean\n“nearly all” values (99.7%) are within -3 and +3 standard deviations of the mean\nthe measure of skewness takes values close to zero (symmetric distribution) (Figure 5.7 b). Particularly, values between −1 and +1 indicate an approximate bell-shaped curve.\nthe measure of excess kurtosis is close to 0 (mesokurtic) (Figure 5.8 b). A kurtosis value between −1 and +1 indicates normality.\n\n\n\n\n\n\n\nStatistical moments and normality\n\n\n\nThere are two ways in which samples can deviate from normality: skew and kurtosis.\nSkewness\nSkewness is usually described as a measure of a dataset's symmetry – or lack of symmetry. \nSkewness values that are negative indicate a tail to the left (Figure 5.7 a), zero value indicate a symmetric distribution (Figure 5.7 b), while values that are positive indicate a tail to the right (Figure 5.7 c).\nSkewness values between −1 and +1 indicate an approximate bell-shaped curve. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from a bell shape with >1 indicating moderate skewness and >2 indicating severe skewness. Any values above +3 or below−3 are a good indication that the variable is not normally distributed.\n\n\n\n\nFigure 5.7: A distribution can be (a) skewed to the left, (b) symmetric, or (c) skewed to the right.\n\n\n\n\n \nKurtosis\nThe other way that distributions can deviate from normality is kurtosis. The kurtosis parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high or too low).\nDistributions with positive excess kurtosis are called leptokurtic (Figure 5.8 a). If the measure of excess kurtosis is close to 0 the distribution is mesokurtic (Figure 5.8 b). Finally, distributions with negative excess kurtosis are called platykurtic (Figure 5.8 c).\nA kurtosis value between −1 and +1 indicates normality and a value between −1 and −3 or between +1 and +3 indicates a tendency away from normality. Values below −3 or above +3 strongly indicate non-normality.\n\n\n\n\nFigure 5.8: A distribution can be (a) leptokurtic, (b) mesokurtic, or (c) platykurtic."
  },
  {
    "objectID": "two_samples.html",
    "href": "two_samples.html",
    "title": "11  Inference for numerical data: 2 samples",
    "section": "",
    "text": "Assumptions for conducting a Student’s t-test\n\n\n\n\nThe groups are independent\nThe outcome of interest is continuous\nThe data is normally distributed in both groups\nThe data in both groups have similar standard deviations\n\n\n\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{se_{dif}}\\]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Farndon, Lisa J, Wesley Vernon, Stephen J Walters, Simon Dixon, Mike\nBradburn, Michael Concannon, and Julia Potter. 2013. “The\nEffectiveness of Salicylic Acid Plasters Compared with\n‘Usual’ Scalpel Debridement of Corns: A Randomised\nControlled Trial.” Journal of Foot and Ankle Research 6\n(1): 40. https://doi.org/10.1186/1757-1146-6-40."
  }
]