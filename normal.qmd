# Distributions - Normal distribution {#sec-normal}

```{r}
#| include: false

library(tidyverse)
library(here)

library(stevemisc)
library(patchwork)

x <- seq(from=4-3*2, to= 4+3*2, length.out=400) # a vector of size 200 with numbers in [-4,4]
y <- dnorm(x, mean = 4, sd = 2) # y is a vector of the same size as x, with values from the PDF

```

We have mentioned the normal distribution several times in this textbook with the promise we would provide more detail later. In this section, we will explore the normal distribution, why it matters, and how we can tell if our distribution is normal or not.

## Characteristics of Normal Distribution

### Historical background

There are several important probability distributions in statistics. However, the normal distribution might be the most important. A normal distribution is the familiar "bell curve" and it's a way of formalizing a distribution where observations cluster around some central tendency. Observations farther from the central tendency occur less frequently. First, Galileo informally described a normal distribution in 1632 when discussing the random errors from observations of celestial phenomena. However, Galileo existed before the time of differential equations and derivatives. We owe its formalization to Carl Friedrich Gauss, which is why the normal distribution is often called a Gaussian distribution. A very familiar example is the height for adult people that approximates a normal distribution very well.

```{r}
#| echo: false
#| label: fig-dnormal
#| out-width: "100%"
#| fig-align: "center"
#| fig-cap: A distribution can be (a) leptokurtic, (b) mesokurtic, or (c) platykurtic.


knitr::include_graphics(here("images", "dnorm.png"))
```

### The mathimatical type

Gauss' normal distribution, technically a density function, is a distribution defined by two parameters, mean $\mu$ and variance $\sigma^2$. The mean , $\mu$, is a "location parameter", which defines the central tendency. The variance, $\sigma^2$ is the "scale parameter", which defines the width of the distribution and how short the distribution is. It's formally given as follows:

$$ f(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}} $$ {#eq-gauss}

The ensuing distribution will look like this in a simple case where $\mu$ is 0 and $\sigma^2$ is 1.

```{r}
#| echo: false
#| label: fig-simple_normal
#| fig-height: 4.2
#| fig-width: 6.0
#| fig-align: "center"
#| fig-cap: A simple normal density function.

ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  theme_steve() + 
  post_bg() +
  stat_function(fun = dnorm, color="#522D80", size=1.5) +
  labs(title = "A Simple Normal Density Function",
       subtitle = "The mu parameter determines the central tendency and sigma-squared \nparameter determines the width.",
       x = "", y="") +
  theme(plot.title.position = "plot",
        plot.title = element_text(size= 10),
        plot.subtitle = element_text(size= 8))
```

### Individual components of a normal distribution

We can break down individual components of a normal distribution and explain them until they seem more accessible.

**First**, the "kernel" is the part inside the exponent of the above equation (i.e. $\ {-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}$). Observe, for a simple case where $\mu$ = 0 and $\sigma^2$ = 1 that this part becomes $\ -{\frac {1}{2}}x^{2}$ that is a negative parabola (notice the square term). The minus sign just flips the basic parabola $\ {\frac {1}{2}}x^{2}$ downward.

```{r}
#| echo: false
#| label: fig-basic_parabola
#| fig-height: 4.0
#| fig-width: 8.0
#| fig-cap: (a) A basic parabola and (b) a negative parabola.

parab <- function(x) {x^2/2}
parab2 <- function(x) {-x^2/2}


p_parab <- ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = parab, color="#522d80", size=1.5) +
  theme_steve() + 
  post_bg() +
  labs(title="A basic parabola",
       x = "", y="") +
theme(plot.title.position = "plot",
        plot.title = element_text(size= 10),
        plot.subtitle = element_text(size= 8))

p_parab2 <- ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = parab2, color="#522d80", size=1.5) +
  theme_steve() + post_bg() +
  labs(title="A Negative Parabola",
       subtitle = "Notice the height is at 0 because the negative part \nflipped the parabola downward.",
       x = "", y="") +
theme(plot.title.position = "plot",
        plot.title = element_text(size= 10),
        plot.subtitle = element_text(size= 8))


p_parab + p_parab2
```

**Second**, exponentiating the negative parabola ($\  e^{-{\frac {1}{2}}x^{2}}$) makes it asymptote to 0.

```{r}
#| echo: false
#| label: fig-neg_parabola
#| fig-height: 4.2
#| fig-width: 6.0
#| fig-align: "center"
#| fig-cap: An exponentiated negative parabola.



expparab <- function(x) {exp(-x^2/2)}

p_expparab <- ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = expparab, color="#522d80", size=1.5) +
  theme_steve() + 
  post_bg() +
  labs(title="An Exponentiated Negative Parabola",
       subtitle = "Exponentiating squeezes the parabola, adjusts the height, and makes \nthe tails asymptote to 0.",
       x = "", y="") +
theme(plot.title.position = "plot",
        plot.title = element_text(size= 10),
        plot.subtitle = element_text(size= 8))

p_expparab
```

Notice the tails in the above graph are asymptote to 0. "Asymptote" is a fancier way of saying the tails approximate 0 but never touch or surpass 0. One way of thinking about this as we build toward its inferential implications is that deviations farther from the central tendency are increasingly "unlikely".

**Third**, and with the above point in mind, it should be clear that $\ {\frac {1}{\sigma {\sqrt {2\pi }}}}$ will scale the height of the distribution. Observe that in our simple case where $\mu$ is 0 and $\sigma^2$ is 1, the height of the exponentiated parabola is at 1. That gets multiplied by $\ {\frac {1}{\sqrt {2\pi }}}$ to equal about 0.398.

**Fourth**, the normal distribution is perfectly symmetrical. The mean, $\mu$ determines the location of the distribution as well as its central tendency. All three measures of central tendency, the mode (most frequently occurring value), the median (the middlemost value), and the mean (the statistical average), will be the same. It also means a given observation of x will be as far from $\mu$ as -x. Additionally, the statistical moments of skewness and excess kurtosis are zero.

**Fifth**, we noted the normal distribution as a function and not a probability because the probability of any one value is effectively zero.

### Normal Density Plot with Shaded Regions

Importantly, around 68% of the distribution is between one standard unit of $\mu$. Around 90% of the distribution is between 1.645 standard units on either side of $\mu$. Around 95% of the distribution is between about 1.96 standard units on either side of $\mu$. About 99% of the distribution is between 2.58 standard units on either side of $\mu$. So, the probability that x is between 1 on either side of the $\mu$ of 0 is effectively 0.68. The ease of this interpretation is why researchers like to standardize their variables so that the mean is 0 and the standard deviation (i.e. the scale parameter) is 1.

```{r}
#| echo: false
#| label: fig-auc
#| fig-height: 7.2
#| fig-width: 12.0
#| fig-align: "center"
#| fig-cap: The area underneath a normal distribution


normal_dist("#522d80","#F66733") + 
  theme_steve() + post_bg() +
    labs(title = "The Area Underneath a Normal Distribution",
       subtitle = "The tails extend to infinity and are asymptote to zero, but the full domain sums to 1. The 95% of all possible values are within about 1.96 standard units from the mean.",
       y = "Density",
       x = "")
```

The normal distribution appears as a foundation assumption for a lot of quantitative approaches to frequentist statistics.

In summary, the normal density function is technically unbounded. It has just the two parameters that define its location and scale and the tails are asymptote to 0 no matter what the values of $\mu$ and $\sigma^2$ are. This makes the distribution continuous since x can range over the entire line from $\ -\infty$ to $\ +\infty$. Thus, the function does not reveal the probability of x (the probability of any one value is effectively 0). However, the area under the curve is the full domain of the probability space and sums to 1. The probability of selecting a number between two points on the x-axis equals the area under the curve between those two points.

```{r}
#| echo: false
#| label: fig-standard_normal
#| fig-height: 6.0
#| fig-width: 8.0
#| fig-align: "center"
#| fig-cap: Standard normal distribution with a shaded region

x <- seq(from=-3, to= 3, length.out=200) # a vector of size 200 with numbers in [-4,4]
y <- dnorm(x) # y is a vector of the same size as x, with values from the PDF

# The polygon follows the density curve where 1 <= x <= 2
region.x <- x[1 <= x & x <= 2] # subset from x satisfying the condition
region.y <- y[1 <= x & x <= 2] # subset from y satisfying the condition
# We add initial and final segments, which drop down to the Y axis
region.x <- c(region.x[1], region.x, tail(region.x,1))
region.y <- c(0,region.y,0)


plot(x, y, main="Standard Normal Distribution with a Shaded Region",
type='l', ylab="Density", xlab="x",lwd=3,col="#522d80")
abline(h=0) # adds the x-axis to the plot
polygon(region.x, region.y, density=-1, col="#F66733")

```

To find the area between x = 1 and x = 2, we must integrate the @eq-gauss as following:

$$ E(x)=\int_{1}^{2}f(x)dx $$ {#eq-integral}

Â 

## Properties of an approximately normal distribution

In an approximately **bell-shaped (normal)** distribution:

-   the mean, the median and the mode have very close values

-   the histogram is symmetric about the mean

-   "nearly all" values (99.7%) are within -3 and +3 standard deviations of the mean

-   the measure of skewness takes values close to zero (symmetric distribution) (@fig-skew2 b). Particularly, values between â1 and +1 indicate an approximate bell-shaped curve.

-   the measure of excess kurtosis is close to 0 (mesokurtic) (@fig-kurtosis b). A kurtosis value between â1 and +1 indicates normality.

::: {.callout-note icon="false"}
## **Statistical moments and normality**

There are two ways in which samples can deviate from normality: **skew** and **kurtosis**.

**Skewness**

Skewness is usually described as a measure of a dataset's symmetry -- or lack of symmetry.Â 

Skewness values that are **negative** indicate a tail to the **left** (@fig-skew2 a), **zero** value indicate a **symmetric** distribution (@fig-skew2 b), while values that are **positive** indicate a tail to the **right** (@fig-skew2 c).

Skewness values between â1 and +1 indicate an approximate bell-shaped curve. Values from â1 to â3 or from +1 to +3 indicate that the distribution is tending away from a bell shape with \>1 indicating moderate skewness and \>2 indicating severe skewness. Any values above +3 or belowâ3 are a good indication that the variable is not normally distributed.

```{r}
#| echo: false
#| label: fig-skew2
#| out-width: "105%"
#| fig-align: "center"
#| fig-cap: A distribution can be (a) skewed to the left, (b) symmetric, or (c) skewed to the right.


knitr::include_graphics(here("images", "skew2.png"))
```

Â 

**Kurtosis**

The other way that distributions can deviate from normality is **kurtosis**. The kurtosis parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high or too low).


Distributions with **negative** excess kurtosis are called ***platykurtic*** (@fig-kurtosis a). If the measure of excess kurtosis is **close to 0** the distribution is **mesokurtic** (@fig-kurtosis b). Finally, distributions with **positive** excess kurtosis are called ***leptokurtic*** (@fig-kurtosis c).

A kurtosis value between â1 and +1 indicates normality and a value between â1 and â3 or between +1 and +3 indicates a tendency away from normality. Values below â3 or above +3 strongly indicate non-normality.

```{r}
#| echo: false
#| label: fig-kurtosis
#| out-width: "105%"
#| fig-align: "center"
#| fig-cap: A distribution can be (a) platykurtic, (b) mesokurtic, or (c) leptokurtic.


knitr::include_graphics(here("images", "kurtosis.png"))
```
:::
